{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RESNET20-TF_v1_hard_data_augmentation.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1aMLmeRP2ZQcebYEZ_BLvq2Wsckdbmg6p","authorship_tag":"ABX9TyNtaql9Wvo+3CiMX4sxTG/T"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"XHLx5QL0fE2D","colab_type":"code","outputId":"be6fee9c-351a-4285-ec98-46f53478cfdb","executionInfo":{"status":"ok","timestamp":1584295066301,"user_tz":0,"elapsed":3652,"user":{"displayName":"Cristobal Quezada","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheAW7CMdhNrje0z_QGd4l_A3MeX7VxuSnMXR6MyA=s64","userId":"03209850991900153699"}},"colab":{"base_uri":"https://localhost:8080/","height":107}},"source":["%tensorflow_version 1.x\n","import tensorflow as tf\n","from keras.datasets import cifar10\n","import numpy as np\n","import keras\n","from keras.layers import AveragePooling2D, Flatten, Dense\n","from datetime import datetime\n","from numpy.random import seed\n","from sklearn.utils import shuffle\n","from keras.preprocessing.image import ImageDataGenerator\n","from sklearn.model_selection import train_test_split\n","\n","seed(1)\n","tf.random.set_random_seed(seed=2)\n","\n","\n","batch_size = 32\n","epochs = 200\n","data_augmentation = False\n","num_classes = 10\n","\n","subtract_pixel_mean = True\n","\n","n = 3\n","depth = n*6+2 # DEPTH = 20\n","\n","\n","# Load the CIFAR10 data.\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","# Input shape\n","input_shape = x_train.shape[1:]\n","\n","# Nomalize data\n","x_train = x_train.astype('float32')/255\n","x_test = x_test.astype('float32')/255\n","\n","# Substract pixel mean\n","if subtract_pixel_mean:\n","  x_train_mean = np.mean(x_train, axis=0)\n","  x_train -= x_train_mean\n","  x_test -= x_train_mean\n","\n","print('x_train_shape: ', x_train.shape)\n","print('train samples: ', x_train.shape[0])\n","print('test samples: ' , x_test.shape[0])\n","\n","\n","# Convert class vectors to binary class matrices\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","print('y_train shape: ', y_train.shape)\n","\n","def lr_schedule(epoch):\n","    \"\"\"Learning Rate Schedule\n","\n","    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n","    Called automatically every epoch as part of callbacks during training.\n","\n","    # Arguments\n","        epoch (int): The number of epochs\n","\n","    # Returns\n","        lr (float32): learning rate\n","    \"\"\"\n","    lr = 1e-3\n","    if epoch > 120:\n","        lr *= 0.5e-3\n","    elif epoch > 100:\n","        lr *= 1e-3\n","    elif epoch > 80:\n","        lr *= 1e-2\n","    elif epoch > 60:\n","        lr *= 1e-1\n","    print('Learning rate: ', lr)\n","    return lr\n","\n","def conv2d(x, W, strides, padding):\n","  return tf.nn.conv2d(x, W, strides= strides, padding=padding)\n","\n","def weights(kernel_size, in_channels, out_channels, name=None):\n","  return tf.Variable(tf.keras.initializers.he_normal()([kernel_size, kernel_size, in_channels, out_channels]), name=name)\n","\n","def resnet_layer(inputs,\n","                 num_filters=16,\n","                 kernel_size=3,\n","                 strides=1,\n","                 activation='relu',\n","                 batch_normalization=True,\n","                 conv_first=True):\n","  \n","\n","  x=inputs\n","  in_channels = x.get_shape().as_list()[3]\n","  beta = tf.Variable(tf.zeros([num_filters]), name=\"beta\")\n","  if conv_first:\n","    x = conv2d(x,\n","                weights(kernel_size,in_channels,out_channels = num_filters),\n","                strides,\n","                padding = 'SAME')\n","    if batch_normalization:\n","      mean, variance = tf.nn.moments(x, axes=[0,1,2])\n","      x = tf.nn.batch_normalization(x, mean, variance, offset=None, scale=None, variance_epsilon = 0.000)\n","    if activation is not None:\n","      x = tf.nn.relu(x + beta)\n","  else:\n","    # batch normalization first\n","    if batch_normalization:\n","      mean, variance = tf.nn.moments(x, axes=[0,1,2])\n","      x = tf.nn.batch_normalization(x, mean, variance, offset=None, scale=None, variance_epsilon = 0.0001)\n","    if activation is not None:\n","      x = tf.nn.relu(x + beta)\n","    # Then convolution\n","    x = conv2d(x)+ beta\n","\n","  return x\n","\n","\n","\n","def resnet_v1(input_tensor, depth, num_classes=10):\n","  \"\"\"ResNet Version 1 Model builder [a]\n","\n","  Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n","  Last ReLU is after the shortcut connection.\n","  At the beginning of each stage, the feature map size is halved (downsampled)\n","  by a convolutional layer with strides=2, while the number of filters is\n","  doubled. Within each stage, the layers have the same number filters and the\n","  same number of filters.\n","  Features maps sizes:\n","  stage 0: 32x32, 16\n","  stage 1: 16x16, 32\n","  stage 2:  8x8,  64\n","  The Number of parameters of Rest20 is approx 0.27M\n","\n","\n","  # Arguments\n","      input_tensor (tensor): shape of input image tensor\n","      depth (int): number of core convolutional layers\n","      num_classes (int): number of classes (CIFAR10 has 10)\n","\n","  # Returns\n","      model (Model): tensorflow graph\n","  \"\"\"\n","  if (depth - 2) % 6 != 0:\n","      raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n","  # Start model definition.\n","  num_filters = 16\n","  num_res_blocks = int((depth - 2) / 6)\n","\n","  # inputs: placeholder \n","\n","  x = resnet_layer(input_tensor)\n","  # Instantiate the stack of residual units\n","  for stack in range(3):\n","      for res_block in range(num_res_blocks):\n","          strides = 1\n","          if stack > 0 and res_block == 0:  # first layer but not first stack\n","              strides = 2  # downsample\n","          y = resnet_layer(inputs=x,\n","                            num_filters=num_filters,\n","                            strides=strides)\n","          y = resnet_layer(inputs=y,\n","                            num_filters=num_filters,\n","                            activation=None)\n","          if stack > 0 and res_block == 0:  # first layer but not first stack\n","              # linear projection residual shortcut connection to match\n","              # changed dims\n","              x = resnet_layer(inputs=x,\n","                                num_filters=num_filters,\n","                                kernel_size=1,\n","                                strides=strides,\n","                                activation=None,\n","                                batch_normalization=False)\n","          x = keras.layers.add([x, y])\n","          x = tf.nn.relu(x)\n","      num_filters *= 2\n","\n","  # Add classifier on top.\n","  # v1 does not use BN after last shortcut connection-ReLU\n","  x = AveragePooling2D(pool_size=8)(x)\n","  y = Flatten()(x)\n","  outputs = Dense(num_classes,\n","                  kernel_initializer='he_normal')(y)\n","\n","  return outputs, y\n","\n","      \n","\n","      \n","  \n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["x_train_shape:  (50000, 32, 32, 3)\n","train samples:  50000\n","test samples:  10000\n","y_train shape:  (50000, 10)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"A3QN7JaOchT8","colab_type":"code","colab":{}},"source":["def save_graph(name):\n","  root_logdir = \"log\"\n","  logdir = \"{}/run-{}/\".format(root_logdir, name)\n","  file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n","\n","name = 'grafo_RESNET20'\n","save_graph(name)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cGD5Z8yvm2bK","colab_type":"code","colab":{}},"source":["def tensorboard():\n","  !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","  !unzip ngrok-stable-linux-amd64.zip\n","\n","  LOG_DIR = './log'\n","  get_ipython().system_raw(\n","      'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","      .format(LOG_DIR)\n","  )\n","\n","  get_ipython().system_raw('./ngrok http 6006 &')\n","\n","  !curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","      \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4MggT6bTLw_W","colab_type":"code","colab":{}},"source":["# data augmentation\n","datagen = ImageDataGenerator(\n","    # set input mean to 0 over the dataset\n","    featurewise_center=False,\n","    # set each sample mean to 0\n","    samplewise_center=False,\n","    # divide inputs by std of dataset\n","    featurewise_std_normalization=False,\n","    # divide each input by its std\n","    samplewise_std_normalization=False,\n","    # apply ZCA whitening\n","    zca_whitening=False,\n","    # epsilon for ZCA whitening\n","    zca_epsilon=1e-06,\n","    # randomly rotate images in the range (deg 0 to 180)\n","    rotation_range=0,\n","    # randomly shift images horizontally\n","    width_shift_range=0.1,\n","    # randomly shift images vertically\n","    height_shift_range=0.1,\n","    # set range for random shear\n","    shear_range=0.,\n","    # set range for random zoom\n","    zoom_range=0.,\n","    # set range for random channel shifts\n","    channel_shift_range=0.,\n","    # set mode for filling points outside the input boundaries\n","    fill_mode='nearest',\n","    # value used for fill_mode = \"constant\"\n","    cval=0.,\n","    # randomly flip images\n","    horizontal_flip=True,\n","    # randomly flip images\n","    vertical_flip=False,\n","    # set rescaling factor (applied before any other transformation)\n","    rescale=None,\n","    # set function that will be applied on each input\n","    preprocessing_function=None,\n","    # image data format, either \"channels_first\" or \"channels_last\"\n","    data_format=None,\n","    # fraction of images reserved for validation (strictly between 0 and 1)\n","    validation_split=0.0)\n","\n","def data_augmetation(datagen,x_b,y_b):\n","  for x_batch, y_batch in datagen.flow(x_b, y_b, batch_size=x_b.shape[0]):\n","    break\n","  return x_batch, y_batch\n","\n","  \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AML3ReW91Btd","colab_type":"code","colab":{}},"source":["height, width, channels = input_shape\n","\n","def crear_batch(x_train, y_train, i, batch_size):\n","    x_batch = x_train[i*batch_size: (i+1)*batch_size]\n","    y_batch = y_train[i*batch_size:(i+1)*batch_size]\n","    return x_batch, y_batch\n","\n","def evaluar_test(x_test,y_test,x,y,lr, lr_value,accuracy,cost, k=10):\n","  n = len(x_test)\n","  batch = int(n/k)\n","  accuracy_test = 0\n","  loss_test = 0\n","  for i in range(k):\n","    x_batch = x_test[i*batch:(i+1)*batch]\n","    y_batch = y_test[i*batch:(i+1)*batch]\n","    feed_dict_batch_eval = {x: x_batch, y: y_batch, lr: lr_value} \n","    \n","    accuracy_test += accuracy.eval(feed_dict_batch_eval)/k\n","    loss_test += cost.eval(feed_dict_batch_eval)/k\n","  return accuracy_test, loss_test\n","\n","def get_variable_value(tensor_name, sess):\n","  t = tf.get_default_graph().get_tensor_by_name(tensor_name)\n","  r = sess.run(t)\n","  return r\n","\n","\n","def train_cnn(epochs,x_train,y_train, checkpoint=None):    \n","    tf.reset_default_graph()    \n","    # Entrada grafo\n","    start = datetime.now()  \n","    \n","\n","    x = tf.placeholder(tf.float32, shape=(None, height, width, channels))\n","    y = tf.placeholder(tf.float32, [None, num_classes])\n","    lr = tf.placeholder(tf.float32, [])\n","    output, _ = resnet_v1(x, depth, num_classes=10)\n","    y_ = tf.nn.softmax(output, name='prediction')\n","    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y), name='loss')\n","    optimizer = tf.train.AdamOptimizer(learning_rate = lr).minimize(cost)\n","    correct = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1))  \n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n","    \n","    # nodo para inicializar variables\n","    init_op = tf.global_variables_initializer()\n","\n","\n","    #  Cargar pesos\n","    if checkpoint == None:\n","      saver = tf.train.Saver()\n","    else:\n","      variables = [v for v in tf.global_variables()]\n","\n","      vars_to_restore_dict = {}\n","      for v in variables:\n","        vars_to_restore_dict[v.name[:-2]] = v\n","      saver = tf.train.Saver(vars_to_restore_dict)\n","\n","    #file_writer = tf.summary.FileditareWriter(logdir, tf.get_default_graph())\n","    vector_costos_train = np.zeros([epochs])\n","    vector_costos_test = np.zeros([epochs])\n","    vector_acc_test = np.zeros([epochs])\n","    vector_acc_train = np.zeros([epochs])\n","    \n","    \n","\n","    with tf.Session() as sess:\n","        # restaurar pesos\n","        if checkpoint == None:\n","          sess.run(init_op) #cuando se realiza inicializacion aleatoria\n","        else:\n","          saver.restore(sess, checkpoint) # restaurar desde checkpoint\n","\n","        # parametros iniciales \n","        best_epoch_loss = 10000        \n","        best_acc_test = 0\n","        \n","\n","        \n","        \n","        for epoch in range(epochs):\n","            epoch_loss = 0\n","            acc_train = 0\n","            lr_value = lr_schedule(epoch)\n","            # if random state= None, Shuffle randomly each time.\n","            x_train, y_train = shuffle(x_train, y_train, random_state=None)        \n","              \n","\n","            \n","            for i in range(int(len(x_train)/batch_size)):\n","                x_batch, y_batch = crear_batch(x_train,y_train,i,batch_size)\n","                x_batch, y_batch = data_augmetation(datagen,x_batch, y_batch)\n","                feed_dict_batch = {x: x_batch, y: y_batch, lr: lr_value}\n","\n","                _, c = sess.run([optimizer, cost], feed_dict_batch)\n","                feed_dict_batch_eval = {x: x_batch, y: y_batch, lr: lr_value}              \n","                acc_batch = accuracy.eval(feed_dict_batch_eval)\n","                epoch_loss += c\n","                acc_train += acc_batch\n","                \n","            accuracy_test, loss_test =  evaluar_test(x_test,y_test,x,y,lr, lr_value,accuracy,cost,k=10)\n","            \n","\n","            print('Epoch %i de %i Loss train: %.4f Loss test: %.4f Accuracy train: %.4f Accuracy test: %.4f '%(epoch,epochs,epoch_loss/(int(len(x_train)/batch_size)),\n","                                                                                                                   loss_test,\n","                                                                                                                   acc_train/(int(len(x_train)/batch_size)),\n","                                                                                                                   accuracy_test))         \n","            vector_costos_train[epoch] = epoch_loss/(int(len(x_train)/batch_size))\n","            vector_costos_test[epoch] = loss_test\n","            vector_acc_train[epoch] = acc_train/(int(len(x_train)/batch_size))        \n","            vector_acc_test[epoch] = accuracy_test \n","\n","            if (accuracy_test > best_acc_test):\n","              best_acc_test = accuracy_test\n","              save_path = saver.save(sess, \"checkpoint{}.ckpt\".format(epoch))\n","\n","\n","              \n","                \n","            if (epoch%10 == 0):\n","              difference = datetime.now() - start\n","              print('Tiempo: ',difference)\n","\n","\n","        save_path = saver.save(sess,\"my_model_final.ckpt\")\n","        difference = datetime.now() - start\n","        print('Tiempo: ',difference)\n","        print('El mejor accuracy en test es: ',best_acc_test)\n","      \n","        return vector_costos_train, vector_costos_test, vector_acc_test, vector_acc_train\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UHkT2yKlOCgS","colab_type":"code","colab":{}},"source":["#epochs = 200\n","#train_cnn(epochs,x_train,y_train, checkpoint=None)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AC_2T_1pny1F","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":253},"outputId":"72bd64cd-f0f9-4c7e-a1f0-80c0b3413f34","executionInfo":{"status":"ok","timestamp":1584295074009,"user_tz":0,"elapsed":3111,"user":{"displayName":"Cristobal Quezada","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheAW7CMdhNrje0z_QGd4l_A3MeX7VxuSnMXR6MyA=s64","userId":"03209850991900153699"}}},"source":["x = tf.placeholder(tf.float32, shape=(None, height, width, channels))\n","y = tf.placeholder(tf.float32, [None, num_classes])\n","lr = tf.placeholder(tf.float32, [])\n","output, fv = resnet_v1(x, depth, num_classes=10)\n","y_ = tf.nn.softmax(output, name='prediction')\n","cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y), name='loss')\n","optimizer = tf.train.AdamOptimizer(learning_rate = lr).minimize(cost)\n","correct = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1))  \n","accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n","  "],"execution_count":7,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n","\n","WARNING:tensorflow:From <ipython-input-7-62ed3392b9dc>:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DriNGYY0LMxW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"32c92c94-692a-4cfa-b69d-ec1d598f3f68","executionInfo":{"status":"ok","timestamp":1584295313379,"user_tz":0,"elapsed":237309,"user":{"displayName":"Cristobal Quezada","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheAW7CMdhNrje0z_QGd4l_A3MeX7VxuSnMXR6MyA=s64","userId":"03209850991900153699"}}},"source":["\n","init_op = tf.global_variables_initializer()\n","\n","with tf.Session() as sess:\n","      tf.train.Saver().restore(sess, '/content/drive/My Drive/Memoria_Ingenieria_electrica/Codigo/CODIGO MEMORIA/CNN/RESNET/weights_data_augmentation/checkpoint96.ckpt') # restaurar desde checkpoint\n","      feed_dict_batch_eval = {x: x_train, y: y_train, lr: 0.0}              \n","      fv_eval_train = fv.eval(feed_dict_batch_eval)\n","      feed_dict_batch_eval = {x: x_test, y: y_test, lr: 0.0}              \n","      fv_eval_test = fv.eval(feed_dict_batch_eval)\n","      print(accuracy.eval(feed_dict_batch_eval))\n","    \n","      \n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Memoria_Ingenieria_electrica/Codigo/CODIGO MEMORIA/CNN/RESNET/weights_data_augmentation/checkpoint96.ckpt\n","0.9075\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y_CjuzE-Mlc-","colab_type":"code","colab":{}},"source":["np.save('/content/drive/My Drive/Memoria_Ingenieria_electrica/Codigo/CODIGO MEMORIA/CNN/RESNET/fv_RESNET20_da/fv_RESNET20_da_train.npy', fv_eval_train)\n","np.save('/content/drive/My Drive/Memoria_Ingenieria_electrica/Codigo/CODIGO MEMORIA/CNN/RESNET/fv_RESNET20_da/fv_RESNET20_da_test.npy', fv_eval_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qkXPz5LSsJIe"},"source":["## SVM with feature vector"]},{"cell_type":"code","metadata":{"id":"e_AG9VhEOI3-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":161},"outputId":"a048234e-5686-41e8-8996-84483bc0168b","executionInfo":{"status":"ok","timestamp":1584294471530,"user_tz":0,"elapsed":455766,"user":{"displayName":"Cristobal Quezada","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GheAW7CMdhNrje0z_QGd4l_A3MeX7VxuSnMXR6MyA=s64","userId":"03209850991900153699"}}},"source":["from sklearn.model_selection import train_test_split\n","import datetime\n","from sklearn import svm\n","\n","(_, y_train), (_, y_test) = cifar10.load_data()\n","\n","def extractor_clasificador():\n","\n","  print('data train: ',fv_eval_train.shape)\n","  print('data test: ', fv_eval_test.shape)\n","\n","  X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(fv_eval_train, y_train, test_size=0.33, random_state=42)\n","\n","\n","  C = [0.01,0.1,1, 10,100]\n","  \n","\n","  first_time = datetime.datetime.now()\n","\n","  for c in C:\n","      svc = svm.SVC(kernel = 'rbf', C = c).fit(X_train_a, y_train_a.ravel())  \n","      # model accuracy for X_test   \n","      accuracy = svc.score(X_test_a, y_test_a.ravel())\n","      accuracy_val = svc.score(fv_eval_test, y_test.ravel())\n","      print('Con c: ',c, 'el accuracy validacion es : ', accuracy, 'accuracy test es: ', accuracy_val)\n","\n","  later_time = datetime.datetime.now()\n","  difference = later_time - first_time\n","  print('Tiempo de ejecucion: ', difference)\n","\n","extractor_clasificador()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["data train:  (50000, 64)\n","data test:  (10000, 64)\n","Con c:  0.01 el accuracy validacion es :  0.9733333333333334 accuracy test es:  0.906\n","Con c:  0.1 el accuracy validacion es :  0.9767272727272728 accuracy test es:  0.9092\n","Con c:  1 el accuracy validacion es :  0.9766060606060606 accuracy test es:  0.9075\n","Con c:  10 el accuracy validacion es :  0.9723636363636363 accuracy test es:  0.9025\n","Con c:  100 el accuracy validacion es :  0.967939393939394 accuracy test es:  0.9002\n","Tiempo de ejecucion:  0:04:04.439703\n"],"name":"stdout"}]}]}