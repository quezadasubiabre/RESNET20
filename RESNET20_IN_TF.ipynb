{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de RESNET20-TF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHLx5QL0fE2D",
        "colab_type": "code",
        "outputId": "2b9d946f-1203-4e45-fbae-8eb7af30e917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.layers import AveragePooling2D, Flatten, Dense\n",
        "from datetime import datetime\n",
        "from numpy.random import seed\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "seed(1)\n",
        "tf.random.set_random_seed(seed=2)\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 200\n",
        "data_augmentation = False\n",
        "num_classes = 10\n",
        "\n",
        "subtract_pixel_mean = True\n",
        "\n",
        "n = 3\n",
        "depth = n*6+2 # DEPTH = 20\n",
        "\n",
        "\n",
        "# Load the CIFAR10 data.\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Input shape\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Nomalize data\n",
        "x_train = x_train.astype('float32')/255\n",
        "x_test = x_test.astype('float32')/255\n",
        "\n",
        "# Substract pixel mean\n",
        "if subtract_pixel_mean:\n",
        "  x_train_mean = np.mean(x_train, axis=0)\n",
        "  x_train -= x_train_mean\n",
        "  x_test -= x_train_mean\n",
        "\n",
        "print('x_train_shape: ', x_train.shape)\n",
        "print('train samples: ', x_train.shape[0])\n",
        "print('test samples: ' , x_test.shape[0])\n",
        "print('y_train shape: ', y_train.shape)\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 100:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 70:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 50:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 30:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "def conv2d(x, W, strides, padding):\n",
        "  return tf.nn.conv2d(x, W, strides= strides, padding=padding)\n",
        "\n",
        "def weights(kernel_size, in_channels, out_channels, name=None):\n",
        "  return tf.Variable(tf.keras.initializers.he_normal()([kernel_size, kernel_size, in_channels, out_channels]), name=name)\n",
        "\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "  \n",
        "\n",
        "  x=inputs\n",
        "  in_channels = x.get_shape().as_list()[3]\n",
        "  beta = tf.Variable(tf.zeros([num_filters]), name=\"beta\")\n",
        "  if conv_first:\n",
        "    x = conv2d(x,\n",
        "                weights(kernel_size,in_channels,out_channels = num_filters),\n",
        "                strides,\n",
        "                padding = 'SAME')\n",
        "    if batch_normalization:\n",
        "      mean, variance = tf.nn.moments(x, axes=[0,1,2])\n",
        "      x = tf.nn.batch_normalization(x, mean, variance, offset=None, scale=None, variance_epsilon = 0.000)\n",
        "    if activation is not None:\n",
        "      x = tf.nn.relu(x + beta)\n",
        "  else:\n",
        "    # batch normalization first\n",
        "    if batch_normalization:\n",
        "      mean, variance = tf.nn.moments(x, axes=[0,1,2])\n",
        "      x = tf.nn.batch_normalization(x, mean, variance, offset=None, scale=None, variance_epsilon = 0.0001)\n",
        "    if activation is not None:\n",
        "      x = tf.nn.relu(x + beta)\n",
        "    # Then convolution\n",
        "    x = conv2d(x)+ beta\n",
        "\n",
        "  return x\n",
        "\n",
        "\n",
        "\n",
        "def resnet_v1(input_tensor, depth, num_classes=10):\n",
        "  \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "  Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "  Last ReLU is after the shortcut connection.\n",
        "  At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "  by a convolutional layer with strides=2, while the number of filters is\n",
        "  doubled. Within each stage, the layers have the same number filters and the\n",
        "  same number of filters.\n",
        "  Features maps sizes:\n",
        "  stage 0: 32x32, 16\n",
        "  stage 1: 16x16, 32\n",
        "  stage 2:  8x8,  64\n",
        "  The Number of parameters of Rest20 is approx 0.27M\n",
        "\n",
        "\n",
        "  # Arguments\n",
        "      input_tensor (tensor): shape of input image tensor\n",
        "      depth (int): number of core convolutional layers\n",
        "      num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "  # Returns\n",
        "      model (Model): tensorflow graph\n",
        "  \"\"\"\n",
        "  if (depth - 2) % 6 != 0:\n",
        "      raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "  # Start model definition.\n",
        "  num_filters = 16\n",
        "  num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "  # inputs: placeholder \n",
        "\n",
        "  x = resnet_layer(input_tensor)\n",
        "  # Instantiate the stack of residual units\n",
        "  for stack in range(3):\n",
        "      for res_block in range(num_res_blocks):\n",
        "          strides = 1\n",
        "          if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "              strides = 2  # downsample\n",
        "          y = resnet_layer(inputs=x,\n",
        "                            num_filters=num_filters,\n",
        "                            strides=strides)\n",
        "          y = resnet_layer(inputs=y,\n",
        "                            num_filters=num_filters,\n",
        "                            activation=None)\n",
        "          if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "              # linear projection residual shortcut connection to match\n",
        "              # changed dims\n",
        "              x = resnet_layer(inputs=x,\n",
        "                                num_filters=num_filters,\n",
        "                                kernel_size=1,\n",
        "                                strides=strides,\n",
        "                                activation=None,\n",
        "                                batch_normalization=False)\n",
        "          x = keras.layers.add([x, y])\n",
        "          x = tf.nn.relu(x)\n",
        "      num_filters *= 2\n",
        "\n",
        "  # Add classifier on top.\n",
        "  # v1 does not use BN after last shortcut connection-ReLU\n",
        "  x = AveragePooling2D(pool_size=8)(x)\n",
        "  y = Flatten()(x)\n",
        "  outputs = Dense(num_classes,\n",
        "                  kernel_initializer='he_normal')(y)\n",
        "\n",
        "  return outputs\n",
        "\n",
        "      \n",
        "\n",
        "      \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x_train_shape:  (50000, 32, 32, 3)\n",
            "train samples:  50000\n",
            "test samples:  10000\n",
            "y_train shape:  (50000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3QN7JaOchT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_graph(name):\n",
        "  root_logdir = \"log\"\n",
        "  logdir = \"{}/run-{}/\".format(root_logdir, name)\n",
        "  file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
        "\n",
        "name = 'grafo_RESNET20'\n",
        "save_graph(name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGD5Z8yvm2bK",
        "colab_type": "code",
        "outputId": "c7d89f56-24b9-4c2b-812e-b01d52ceb353",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-10 19:43:15--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.7.1.159, 34.235.245.236, 34.238.36.128, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.7.1.159|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.3’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  13.3MB/s    in 1.0s    \n",
            "\n",
            "2020-03-10 19:43:16 (13.3 MB/s) - ‘ngrok-stable-linux-amd64.zip.3’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n",
            "https://f641d10f.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AML3ReW91Btd",
        "colab_type": "code",
        "outputId": "70e0b104-1e3f-421e-80cc-4ac659ced2a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "height, width, channels = input_shape\n",
        "\n",
        "def crear_batch(x_train, y_train, i, batch_size):\n",
        "    x_batch = x_train[i*batch_size: (i+1)*batch_size]\n",
        "    y_batch = y_train[i*batch_size:(i+1)*batch_size]\n",
        "    return x_batch, y_batch\n",
        "\n",
        "def evaluar_test(x_test,y_test,x,y,lr, lr_value,accuracy,cost, k=10):\n",
        "  n = len(x_test)\n",
        "  batch = int(n/k)\n",
        "  accuracy_test = 0\n",
        "  loss_test = 0\n",
        "  for i in range(k):\n",
        "    x_batch = x_test[i*batch:(i+1)*batch]\n",
        "    y_batch = y_test[i*batch:(i+1)*batch]\n",
        "    feed_dict_batch_eval = {x: x_batch, y: y_batch, lr: lr_value} \n",
        "    \n",
        "    accuracy_test += accuracy.eval(feed_dict_batch_eval)/k\n",
        "    loss_test += cost.eval(feed_dict_batch_eval)/k\n",
        "  return accuracy_test, loss_test\n",
        "\n",
        "def get_variable_value(tensor_name, sess):\n",
        "  t = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
        "  r = sess.run(t)\n",
        "  return r\n",
        "\n",
        "\n",
        "def train_cnn(epochs,x_train,y_train, checkpoint=None):    \n",
        "    tf.reset_default_graph()    \n",
        "    # Entrada grafo\n",
        "    start = datetime.now()  \n",
        "    \n",
        "\n",
        "    x = tf.placeholder(tf.float32, shape=(None, height, width, channels))\n",
        "    y = tf.placeholder(tf.float32, [None, num_classes])\n",
        "    lr = tf.placeholder(tf.float32, [])\n",
        "    output = resnet_v1(x, depth, num_classes=10)\n",
        "    y_ = tf.nn.softmax(output, name='prediction')\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y), name='loss')\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate = lr).minimize(cost)\n",
        "    correct = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1))  \n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
        "    \n",
        "    # nodo para inicializar variables\n",
        "    init_op = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "    #  Cargar pesos\n",
        "    if checkpoint == None:\n",
        "      saver = tf.train.Saver()\n",
        "    else:\n",
        "      variables = [v for v in tf.global_variables()]\n",
        "\n",
        "      vars_to_restore_dict = {}\n",
        "      for v in variables:\n",
        "        vars_to_restore_dict[v.name[:-2]] = v\n",
        "      saver = tf.train.Saver(vars_to_restore_dict)\n",
        "\n",
        "    #file_writer = tf.summary.FileditareWriter(logdir, tf.get_default_graph())\n",
        "    vector_costos_train = np.zeros([epochs])\n",
        "    vector_costos_test = np.zeros([epochs])\n",
        "    vector_acc_test = np.zeros([epochs])\n",
        "    vector_acc_train = np.zeros([epochs])\n",
        "    \n",
        "    \n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        # restaurar pesos\n",
        "        if checkpoint == None:\n",
        "          sess.run(init_op) #cuando se realiza inicializacion aleatoria\n",
        "        else:\n",
        "          saver.restore(sess, checkpoint) # restaurar desde checkpoint\n",
        "\n",
        "        # parametros iniciales \n",
        "        best_epoch_loss = 10000        \n",
        "        best_acc_test = 0\n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            acc_train = 0\n",
        "            lr_value = lr_schedule(epoch)\n",
        "            # if random state= None, Shuffle randomly each time.\n",
        "            x_train, y_train = shuffle(x_train, y_train, random_state=None)        \n",
        "              \n",
        "\n",
        "            \n",
        "            for i in range(int(len(x_train)/batch_size)):\n",
        "                x_batch, y_batch = crear_batch(x_train,y_train,i,batch_size)\n",
        "                feed_dict_batch = {x: x_batch, y: y_batch, lr: lr_value}\n",
        "\n",
        "                _, c = sess.run([optimizer, cost], feed_dict_batch)\n",
        "                feed_dict_batch_eval = {x: x_batch, y: y_batch, lr: lr_value}              \n",
        "                acc_batch = accuracy.eval(feed_dict_batch_eval)\n",
        "                epoch_loss += c\n",
        "                acc_train += acc_batch\n",
        "                \n",
        "            accuracy_test, loss_test =  evaluar_test(x_test,y_test,x,y,lr, lr_value,accuracy,cost,k=10)\n",
        "            \n",
        "\n",
        "            print('Epoch %i de %i Loss train: %.4f Loss test: %.4f Accuracy train: %.4f Accuracy test: %.4f '%(epoch,epochs,epoch_loss/(int(len(x_train)/batch_size)),\n",
        "                                                                                                                   loss_test,\n",
        "                                                                                                                   acc_train/(int(len(x_train)/batch_size)),\n",
        "                                                                                                                   accuracy_test))         \n",
        "            vector_costos_train[epoch] = epoch_loss/(int(len(x_train)/batch_size))\n",
        "            vector_costos_test[epoch] = loss_test\n",
        "            vector_acc_train[epoch] = acc_train/(int(len(x_train)/batch_size))        \n",
        "            vector_acc_test[epoch] = accuracy_test \n",
        "\n",
        "            if (accuracy_test > best_acc_test):\n",
        "              best_acc_test = accuracy_test\n",
        "\n",
        "\n",
        "              if (epoch%20 == 0):\n",
        "                save_path = saver.save(sess, \"checkpoint{}.ckpt\".format(epoch))\n",
        "            if (epoch%10 == 0):\n",
        "              difference = datetime.now() - start\n",
        "              print('Tiempo: ',difference)\n",
        "\n",
        "\n",
        "        save_path = saver.save(sess,\"my_model_final.ckpt\")\n",
        "        difference = datetime.now() - start\n",
        "        print('Tiempo: ',difference)\n",
        "        print('El mejor accuracy en test es: ',best_acc_test)\n",
        "      \n",
        "        return vector_costos_train, vector_costos_test, vector_acc_test, vector_acc_train\n",
        "\n",
        "epochs = 100\n",
        "train_cnn(epochs,x_train,y_train,checkpoint=None)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-4-3d78181996ec>:39: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Learning rate:  0.001\n",
            "Epoch 0 de 100 Loss train: 1.3338 Loss test: 0.9679 Accuracy train: 0.5519 Accuracy test: 0.6547 \n",
            "Tiempo:  0:01:45.521389\n",
            "Learning rate:  0.001\n",
            "Epoch 1 de 100 Loss train: 0.9054 Loss test: 0.7877 Accuracy train: 0.7089 Accuracy test: 0.7273 \n",
            "Learning rate:  0.001\n",
            "Epoch 2 de 100 Loss train: 0.7281 Loss test: 0.6735 Accuracy train: 0.7746 Accuracy test: 0.7655 \n",
            "Learning rate:  0.001\n",
            "Epoch 3 de 100 Loss train: 0.6199 Loss test: 0.6237 Accuracy train: 0.8135 Accuracy test: 0.7828 \n",
            "Learning rate:  0.001\n",
            "Epoch 4 de 100 Loss train: 0.5406 Loss test: 0.6124 Accuracy train: 0.8424 Accuracy test: 0.7904 \n",
            "Learning rate:  0.001\n",
            "Epoch 5 de 100 Loss train: 0.4743 Loss test: 0.5894 Accuracy train: 0.8675 Accuracy test: 0.8025 \n",
            "Learning rate:  0.001\n",
            "Epoch 6 de 100 Loss train: 0.4223 Loss test: 0.5729 Accuracy train: 0.8849 Accuracy test: 0.8039 \n",
            "Learning rate:  0.001\n",
            "Epoch 7 de 100 Loss train: 0.3698 Loss test: 0.5684 Accuracy train: 0.9030 Accuracy test: 0.8064 \n",
            "Learning rate:  0.001\n",
            "Epoch 8 de 100 Loss train: 0.3298 Loss test: 0.5694 Accuracy train: 0.9193 Accuracy test: 0.8118 \n",
            "Learning rate:  0.001\n",
            "Epoch 9 de 100 Loss train: 0.2864 Loss test: 0.5812 Accuracy train: 0.9358 Accuracy test: 0.8096 \n",
            "Learning rate:  0.001\n",
            "Epoch 10 de 100 Loss train: 0.2541 Loss test: 0.5909 Accuracy train: 0.9463 Accuracy test: 0.8123 \n",
            "Tiempo:  0:18:08.716500\n",
            "Learning rate:  0.001\n",
            "Epoch 11 de 100 Loss train: 0.2241 Loss test: 0.6102 Accuracy train: 0.9571 Accuracy test: 0.8162 \n",
            "Learning rate:  0.001\n",
            "Epoch 12 de 100 Loss train: 0.2010 Loss test: 0.6271 Accuracy train: 0.9660 Accuracy test: 0.8093 \n",
            "Learning rate:  0.001\n",
            "Epoch 13 de 100 Loss train: 0.1743 Loss test: 0.6669 Accuracy train: 0.9735 Accuracy test: 0.8043 \n",
            "Learning rate:  0.001\n",
            "Epoch 14 de 100 Loss train: 0.1608 Loss test: 0.6773 Accuracy train: 0.9778 Accuracy test: 0.8111 \n",
            "Learning rate:  0.001\n",
            "Epoch 15 de 100 Loss train: 0.1457 Loss test: 0.6798 Accuracy train: 0.9815 Accuracy test: 0.8105 \n",
            "Learning rate:  0.001\n",
            "Epoch 16 de 100 Loss train: 0.1310 Loss test: 0.7022 Accuracy train: 0.9846 Accuracy test: 0.8087 \n",
            "Learning rate:  0.001\n",
            "Epoch 17 de 100 Loss train: 0.1213 Loss test: 0.7286 Accuracy train: 0.9879 Accuracy test: 0.8043 \n",
            "Learning rate:  0.001\n",
            "Epoch 18 de 100 Loss train: 0.1147 Loss test: 0.7677 Accuracy train: 0.9892 Accuracy test: 0.7993 \n",
            "Learning rate:  0.001\n",
            "Epoch 19 de 100 Loss train: 0.1026 Loss test: 0.7591 Accuracy train: 0.9918 Accuracy test: 0.8074 \n",
            "Learning rate:  0.001\n",
            "Epoch 20 de 100 Loss train: 0.1015 Loss test: 0.7833 Accuracy train: 0.9921 Accuracy test: 0.8095 \n",
            "Tiempo:  0:34:25.440454\n",
            "Learning rate:  0.001\n",
            "Epoch 21 de 100 Loss train: 0.0926 Loss test: 0.7780 Accuracy train: 0.9936 Accuracy test: 0.8073 \n",
            "Learning rate:  0.001\n",
            "Epoch 22 de 100 Loss train: 0.0884 Loss test: 0.7880 Accuracy train: 0.9944 Accuracy test: 0.8101 \n",
            "Learning rate:  0.001\n",
            "Epoch 23 de 100 Loss train: 0.0819 Loss test: 0.8197 Accuracy train: 0.9955 Accuracy test: 0.8026 \n",
            "Learning rate:  0.001\n",
            "Epoch 24 de 100 Loss train: 0.0847 Loss test: 0.7798 Accuracy train: 0.9950 Accuracy test: 0.8096 \n",
            "Learning rate:  0.001\n",
            "Epoch 25 de 100 Loss train: 0.0768 Loss test: 0.7993 Accuracy train: 0.9959 Accuracy test: 0.8101 \n",
            "Learning rate:  0.001\n",
            "Epoch 26 de 100 Loss train: 0.0736 Loss test: 0.8187 Accuracy train: 0.9962 Accuracy test: 0.8093 \n",
            "Learning rate:  0.001\n",
            "Epoch 27 de 100 Loss train: 0.0763 Loss test: 0.8007 Accuracy train: 0.9957 Accuracy test: 0.8109 \n",
            "Learning rate:  0.001\n",
            "Epoch 28 de 100 Loss train: 0.0692 Loss test: 0.8398 Accuracy train: 0.9966 Accuracy test: 0.8051 \n",
            "Learning rate:  0.001\n",
            "Epoch 29 de 100 Loss train: 0.0647 Loss test: 0.8480 Accuracy train: 0.9972 Accuracy test: 0.8160 \n",
            "Learning rate:  0.001\n",
            "Epoch 30 de 100 Loss train: 0.0661 Loss test: 0.8533 Accuracy train: 0.9972 Accuracy test: 0.8070 \n",
            "Tiempo:  0:50:37.645762\n",
            "Learning rate:  0.0001\n",
            "Epoch 31 de 100 Loss train: 0.0265 Loss test: 0.7858 Accuracy train: 0.9934 Accuracy test: 0.8212 \n",
            "Learning rate:  0.0001\n",
            "Epoch 32 de 100 Loss train: 0.0137 Loss test: 0.7883 Accuracy train: 0.9980 Accuracy test: 0.8253 \n",
            "Learning rate:  0.0001\n",
            "Epoch 33 de 100 Loss train: 0.0091 Loss test: 0.7989 Accuracy train: 0.9994 Accuracy test: 0.8239 \n",
            "Learning rate:  0.0001\n",
            "Epoch 34 de 100 Loss train: 0.0079 Loss test: 0.7991 Accuracy train: 0.9996 Accuracy test: 0.8275 \n",
            "Learning rate:  0.0001\n",
            "Epoch 35 de 100 Loss train: 0.0064 Loss test: 0.8140 Accuracy train: 0.9999 Accuracy test: 0.8250 \n",
            "Learning rate:  0.0001\n",
            "Epoch 36 de 100 Loss train: 0.0055 Loss test: 0.8247 Accuracy train: 0.9999 Accuracy test: 0.8261 \n",
            "Learning rate:  0.0001\n",
            "Epoch 37 de 100 Loss train: 0.0048 Loss test: 0.8272 Accuracy train: 0.9999 Accuracy test: 0.8275 \n",
            "Learning rate:  0.0001\n",
            "Epoch 38 de 100 Loss train: 0.0042 Loss test: 0.8434 Accuracy train: 1.0000 Accuracy test: 0.8253 \n",
            "Learning rate:  0.0001\n",
            "Epoch 39 de 100 Loss train: 0.0041 Loss test: 0.8458 Accuracy train: 1.0000 Accuracy test: 0.8260 \n",
            "Learning rate:  0.0001\n",
            "Epoch 40 de 100 Loss train: 0.0040 Loss test: 0.8611 Accuracy train: 0.9999 Accuracy test: 0.8234 \n",
            "Tiempo:  1:07:01.505891\n",
            "Learning rate:  0.0001\n",
            "Epoch 41 de 100 Loss train: 0.0032 Loss test: 0.8604 Accuracy train: 1.0000 Accuracy test: 0.8260 \n",
            "Learning rate:  0.0001\n",
            "Epoch 42 de 100 Loss train: 0.0031 Loss test: 0.8690 Accuracy train: 1.0000 Accuracy test: 0.8269 \n",
            "Learning rate:  0.0001\n",
            "Epoch 43 de 100 Loss train: 0.0029 Loss test: 0.8709 Accuracy train: 1.0000 Accuracy test: 0.8271 \n",
            "Learning rate:  0.0001\n",
            "Epoch 44 de 100 Loss train: 0.0030 Loss test: 0.8762 Accuracy train: 1.0000 Accuracy test: 0.8258 \n",
            "Learning rate:  0.0001\n",
            "Epoch 45 de 100 Loss train: 0.0025 Loss test: 0.8831 Accuracy train: 1.0000 Accuracy test: 0.8268 \n",
            "Learning rate:  0.0001\n",
            "Epoch 46 de 100 Loss train: 0.0025 Loss test: 0.8930 Accuracy train: 1.0000 Accuracy test: 0.8285 \n",
            "Learning rate:  0.0001\n",
            "Epoch 47 de 100 Loss train: 0.0027 Loss test: 0.9009 Accuracy train: 1.0000 Accuracy test: 0.8243 \n",
            "Learning rate:  0.0001\n",
            "Epoch 48 de 100 Loss train: 0.0024 Loss test: 0.8993 Accuracy train: 1.0000 Accuracy test: 0.8268 \n",
            "Learning rate:  0.0001\n",
            "Epoch 49 de 100 Loss train: 0.0022 Loss test: 0.9081 Accuracy train: 1.0000 Accuracy test: 0.8258 \n",
            "Learning rate:  0.0001\n",
            "Epoch 50 de 100 Loss train: 0.0024 Loss test: 0.9070 Accuracy train: 1.0000 Accuracy test: 0.8257 \n",
            "Tiempo:  1:23:17.035502\n",
            "Learning rate:  1e-05\n",
            "Epoch 51 de 100 Loss train: 0.0019 Loss test: 0.9048 Accuracy train: 0.9998 Accuracy test: 0.8283 \n",
            "Learning rate:  1e-05\n",
            "Epoch 52 de 100 Loss train: 0.0017 Loss test: 0.9036 Accuracy train: 0.9998 Accuracy test: 0.8295 \n",
            "Learning rate:  1e-05\n",
            "Epoch 53 de 100 Loss train: 0.0016 Loss test: 0.9041 Accuracy train: 0.9999 Accuracy test: 0.8292 \n",
            "Learning rate:  1e-05\n",
            "Epoch 54 de 100 Loss train: 0.0015 Loss test: 0.9059 Accuracy train: 0.9999 Accuracy test: 0.8287 \n",
            "Learning rate:  1e-05\n",
            "Epoch 55 de 100 Loss train: 0.0014 Loss test: 0.9066 Accuracy train: 0.9999 Accuracy test: 0.8287 \n",
            "Learning rate:  1e-05\n",
            "Epoch 56 de 100 Loss train: 0.0014 Loss test: 0.9063 Accuracy train: 1.0000 Accuracy test: 0.8300 \n",
            "Learning rate:  1e-05\n",
            "Epoch 57 de 100 Loss train: 0.0015 Loss test: 0.9064 Accuracy train: 0.9999 Accuracy test: 0.8307 \n",
            "Learning rate:  1e-05\n",
            "Epoch 58 de 100 Loss train: 0.0013 Loss test: 0.9065 Accuracy train: 1.0000 Accuracy test: 0.8299 \n",
            "Learning rate:  1e-05\n",
            "Epoch 59 de 100 Loss train: 0.0014 Loss test: 0.9080 Accuracy train: 0.9999 Accuracy test: 0.8293 \n",
            "Learning rate:  1e-05\n",
            "Epoch 60 de 100 Loss train: 0.0012 Loss test: 0.9073 Accuracy train: 1.0000 Accuracy test: 0.8302 \n",
            "Tiempo:  1:39:20.561166\n",
            "Learning rate:  1e-05\n",
            "Epoch 61 de 100 Loss train: 0.0013 Loss test: 0.9067 Accuracy train: 0.9999 Accuracy test: 0.8295 \n",
            "Learning rate:  1e-05\n",
            "Epoch 62 de 100 Loss train: 0.0011 Loss test: 0.9081 Accuracy train: 1.0000 Accuracy test: 0.8291 \n",
            "Learning rate:  1e-05\n",
            "Epoch 63 de 100 Loss train: 0.0012 Loss test: 0.9069 Accuracy train: 1.0000 Accuracy test: 0.8293 \n",
            "Learning rate:  1e-05\n",
            "Epoch 64 de 100 Loss train: 0.0012 Loss test: 0.9074 Accuracy train: 0.9999 Accuracy test: 0.8293 \n",
            "Learning rate:  1e-05\n",
            "Epoch 65 de 100 Loss train: 0.0011 Loss test: 0.9075 Accuracy train: 1.0000 Accuracy test: 0.8288 \n",
            "Learning rate:  1e-05\n",
            "Epoch 66 de 100 Loss train: 0.0011 Loss test: 0.9103 Accuracy train: 1.0000 Accuracy test: 0.8298 \n",
            "Learning rate:  1e-05\n",
            "Epoch 67 de 100 Loss train: 0.0012 Loss test: 0.9126 Accuracy train: 1.0000 Accuracy test: 0.8281 \n",
            "Learning rate:  1e-05\n",
            "Epoch 68 de 100 Loss train: 0.0011 Loss test: 0.9124 Accuracy train: 0.9999 Accuracy test: 0.8290 \n",
            "Learning rate:  1e-05\n",
            "Epoch 69 de 100 Loss train: 0.0010 Loss test: 0.9114 Accuracy train: 1.0000 Accuracy test: 0.8291 \n",
            "Learning rate:  1e-05\n",
            "Epoch 70 de 100 Loss train: 0.0011 Loss test: 0.9118 Accuracy train: 1.0000 Accuracy test: 0.8295 \n",
            "Tiempo:  1:55:28.597791\n",
            "Learning rate:  1e-06\n",
            "Epoch 71 de 100 Loss train: 0.0011 Loss test: 0.9119 Accuracy train: 0.9999 Accuracy test: 0.8297 \n",
            "Learning rate:  1e-06\n",
            "Epoch 72 de 100 Loss train: 0.0010 Loss test: 0.9121 Accuracy train: 1.0000 Accuracy test: 0.8299 \n",
            "Learning rate:  1e-06\n",
            "Epoch 73 de 100 Loss train: 0.0011 Loss test: 0.9119 Accuracy train: 0.9999 Accuracy test: 0.8297 \n",
            "Learning rate:  1e-06\n",
            "Epoch 74 de 100 Loss train: 0.0010 Loss test: 0.9121 Accuracy train: 1.0000 Accuracy test: 0.8298 \n",
            "Learning rate:  1e-06\n",
            "Epoch 75 de 100 Loss train: 0.0011 Loss test: 0.9121 Accuracy train: 0.9999 Accuracy test: 0.8296 \n",
            "Learning rate:  1e-06\n",
            "Epoch 76 de 100 Loss train: 0.0011 Loss test: 0.9123 Accuracy train: 0.9999 Accuracy test: 0.8295 \n",
            "Learning rate:  1e-06\n",
            "Epoch 77 de 100 Loss train: 0.0010 Loss test: 0.9123 Accuracy train: 1.0000 Accuracy test: 0.8300 \n",
            "Learning rate:  1e-06\n",
            "Epoch 78 de 100 Loss train: 0.0009 Loss test: 0.9125 Accuracy train: 1.0000 Accuracy test: 0.8297 \n",
            "Learning rate:  1e-06\n",
            "Epoch 79 de 100 Loss train: 0.0011 Loss test: 0.9128 Accuracy train: 1.0000 Accuracy test: 0.8296 \n",
            "Learning rate:  1e-06\n",
            "Epoch 80 de 100 Loss train: 0.0011 Loss test: 0.9127 Accuracy train: 1.0000 Accuracy test: 0.8298 \n",
            "Tiempo:  2:11:41.552166\n",
            "Learning rate:  1e-06\n",
            "Epoch 81 de 100 Loss train: 0.0010 Loss test: 0.9126 Accuracy train: 1.0000 Accuracy test: 0.8297 \n",
            "Learning rate:  1e-06\n",
            "Epoch 82 de 100 Loss train: 0.0011 Loss test: 0.9127 Accuracy train: 1.0000 Accuracy test: 0.8297 \n",
            "Learning rate:  1e-06\n",
            "Epoch 83 de 100 Loss train: 0.0009 Loss test: 0.9127 Accuracy train: 1.0000 Accuracy test: 0.8297 \n",
            "Learning rate:  1e-06\n",
            "Epoch 84 de 100 Loss train: 0.0013 Loss test: 0.9127 Accuracy train: 0.9999 Accuracy test: 0.8298 \n",
            "Learning rate:  1e-06\n",
            "Epoch 85 de 100 Loss train: 0.0010 Loss test: 0.9128 Accuracy train: 1.0000 Accuracy test: 0.8297 \n",
            "Learning rate:  1e-06\n",
            "Epoch 86 de 100 Loss train: 0.0010 Loss test: 0.9127 Accuracy train: 1.0000 Accuracy test: 0.8295 \n",
            "Learning rate:  1e-06\n",
            "Epoch 87 de 100 Loss train: 0.0011 Loss test: 0.9127 Accuracy train: 0.9999 Accuracy test: 0.8297 \n",
            "Learning rate:  1e-06\n",
            "Epoch 88 de 100 Loss train: 0.0011 Loss test: 0.9127 Accuracy train: 0.9999 Accuracy test: 0.8297 \n",
            "Learning rate:  1e-06\n",
            "Epoch 89 de 100 Loss train: 0.0011 Loss test: 0.9128 Accuracy train: 1.0000 Accuracy test: 0.8296 \n",
            "Learning rate:  1e-06\n",
            "Epoch 90 de 100 Loss train: 0.0011 Loss test: 0.9129 Accuracy train: 1.0000 Accuracy test: 0.8291 \n",
            "Tiempo:  2:27:55.383207\n",
            "Learning rate:  1e-06\n",
            "Epoch 91 de 100 Loss train: 0.0010 Loss test: 0.9129 Accuracy train: 1.0000 Accuracy test: 0.8295 \n",
            "Learning rate:  1e-06\n",
            "Epoch 92 de 100 Loss train: 0.0010 Loss test: 0.9131 Accuracy train: 1.0000 Accuracy test: 0.8297 \n",
            "Learning rate:  1e-06\n",
            "Epoch 93 de 100 Loss train: 0.0011 Loss test: 0.9130 Accuracy train: 0.9999 Accuracy test: 0.8298 \n",
            "Learning rate:  1e-06\n",
            "Epoch 94 de 100 Loss train: 0.0010 Loss test: 0.9131 Accuracy train: 1.0000 Accuracy test: 0.8300 \n",
            "Learning rate:  1e-06\n",
            "Epoch 95 de 100 Loss train: 0.0009 Loss test: 0.9132 Accuracy train: 1.0000 Accuracy test: 0.8300 \n",
            "Learning rate:  1e-06\n",
            "Epoch 96 de 100 Loss train: 0.0010 Loss test: 0.9135 Accuracy train: 1.0000 Accuracy test: 0.8293 \n",
            "Learning rate:  1e-06\n",
            "Epoch 97 de 100 Loss train: 0.0010 Loss test: 0.9136 Accuracy train: 1.0000 Accuracy test: 0.8294 \n",
            "Learning rate:  1e-06\n",
            "Epoch 98 de 100 Loss train: 0.0009 Loss test: 0.9135 Accuracy train: 1.0000 Accuracy test: 0.8295 \n",
            "Learning rate:  1e-06\n",
            "Epoch 99 de 100 Loss train: 0.0011 Loss test: 0.9135 Accuracy train: 0.9999 Accuracy test: 0.8299 \n",
            "Tiempo:  2:42:32.817415\n",
            "El mejor accuracy en test es:  0.8306999921798706\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1.33384981e+00, 9.05369921e-01, 7.28116039e-01, 6.19905423e-01,\n",
              "        5.40611348e-01, 4.74278541e-01, 4.22305107e-01, 3.69843254e-01,\n",
              "        3.29785784e-01, 2.86415298e-01, 2.54113483e-01, 2.24112560e-01,\n",
              "        2.01029631e-01, 1.74344460e-01, 1.60769662e-01, 1.45675276e-01,\n",
              "        1.31030302e-01, 1.21267690e-01, 1.14659804e-01, 1.02592555e-01,\n",
              "        1.01464268e-01, 9.26420918e-02, 8.84184317e-02, 8.19119577e-02,\n",
              "        8.46886064e-02, 7.68456328e-02, 7.35747480e-02, 7.62602972e-02,\n",
              "        6.91873797e-02, 6.46982228e-02, 6.61366651e-02, 2.65270634e-02,\n",
              "        1.37174857e-02, 9.10273632e-03, 7.91029138e-03, 6.38902992e-03,\n",
              "        5.45134426e-03, 4.78559349e-03, 4.19724695e-03, 4.14793363e-03,\n",
              "        3.96109259e-03, 3.16104703e-03, 3.08417972e-03, 2.89388104e-03,\n",
              "        2.99820751e-03, 2.51811940e-03, 2.45287902e-03, 2.69363319e-03,\n",
              "        2.44851880e-03, 2.15253313e-03, 2.35443030e-03, 1.87121169e-03,\n",
              "        1.66333314e-03, 1.61559730e-03, 1.52655016e-03, 1.41261990e-03,\n",
              "        1.38337811e-03, 1.49496513e-03, 1.29957358e-03, 1.40683779e-03,\n",
              "        1.24192030e-03, 1.27564595e-03, 1.11416938e-03, 1.21390083e-03,\n",
              "        1.18934527e-03, 1.13714215e-03, 1.07507667e-03, 1.16630077e-03,\n",
              "        1.09887023e-03, 1.01599776e-03, 1.07804073e-03, 1.07399693e-03,\n",
              "        9.55708064e-04, 1.10232845e-03, 1.04805099e-03, 1.10542385e-03,\n",
              "        1.05540970e-03, 1.04387071e-03, 9.45026450e-04, 1.05681212e-03,\n",
              "        1.10723067e-03, 9.93551092e-04, 1.06716895e-03, 8.96479589e-04,\n",
              "        1.26815172e-03, 9.67467102e-04, 1.02989167e-03, 1.08131475e-03,\n",
              "        1.09056788e-03, 1.07717881e-03, 1.08051350e-03, 9.61806080e-04,\n",
              "        1.00857987e-03, 1.12862423e-03, 1.02416574e-03, 9.18876124e-04,\n",
              "        1.04967079e-03, 1.03193743e-03, 8.96354617e-04, 1.05912241e-03]),\n",
              " array([0.96786554, 0.78770803, 0.67349027, 0.62365654, 0.61239938,\n",
              "        0.5894241 , 0.57291653, 0.56841521, 0.56938535, 0.58121065,\n",
              "        0.59092208, 0.61017746, 0.62713159, 0.66692057, 0.67731743,\n",
              "        0.67981644, 0.7022182 , 0.72857198, 0.76767455, 0.75911167,\n",
              "        0.78334782, 0.77797168, 0.78797814, 0.81973535, 0.77979593,\n",
              "        0.79925861, 0.81866007, 0.80065373, 0.83982897, 0.84803425,\n",
              "        0.85330985, 0.78584548, 0.78828695, 0.79886579, 0.79910979,\n",
              "        0.81403399, 0.82469802, 0.8271592 , 0.84339325, 0.8457983 ,\n",
              "        0.8610986 , 0.86035313, 0.86902289, 0.87094182, 0.87616159,\n",
              "        0.88308842, 0.89301969, 0.90094302, 0.89927108, 0.90805838,\n",
              "        0.90697257, 0.90475386, 0.90363573, 0.90407377, 0.90591635,\n",
              "        0.90657675, 0.90629426, 0.90640314, 0.90654942, 0.90800141,\n",
              "        0.90728721, 0.90667699, 0.90813532, 0.90691884, 0.90743366,\n",
              "        0.90750567, 0.91030494, 0.91262351, 0.9123506 , 0.91143526,\n",
              "        0.91179681, 0.91186414, 0.91205786, 0.91190943, 0.91205631,\n",
              "        0.91208496, 0.9122563 , 0.91234004, 0.91253765, 0.91278431,\n",
              "        0.91270314, 0.91261501, 0.91268651, 0.91270453, 0.91271139,\n",
              "        0.91279663, 0.91273529, 0.91268605, 0.91272134, 0.91279774,\n",
              "        0.91289998, 0.91294769, 0.91314282, 0.91303433, 0.91308374,\n",
              "        0.91317642, 0.91347175, 0.91363401, 0.91352499, 0.91349757]),\n",
              " array([0.6547    , 0.72729999, 0.7655    , 0.78280001, 0.7904    ,\n",
              "        0.8025    , 0.8039    , 0.8064    , 0.8118    , 0.8096    ,\n",
              "        0.8123    , 0.8162    , 0.80929999, 0.80429999, 0.81110001,\n",
              "        0.81049999, 0.8087    , 0.8043    , 0.7993    , 0.80740001,\n",
              "        0.80950001, 0.8073    , 0.8101    , 0.80260001, 0.8096    ,\n",
              "        0.81010001, 0.8093    , 0.8109    , 0.8051    , 0.816     ,\n",
              "        0.80700001, 0.8212    , 0.82529999, 0.8239    , 0.8275    ,\n",
              "        0.825     , 0.8261    , 0.8275    , 0.8253    , 0.82599999,\n",
              "        0.82340001, 0.826     , 0.8269    , 0.8271    , 0.8258    ,\n",
              "        0.82680001, 0.8285    , 0.8243    , 0.82679999, 0.8258    ,\n",
              "        0.8257    , 0.82829999, 0.8295    , 0.82920001, 0.8287    ,\n",
              "        0.8287    , 0.83      , 0.83069999, 0.8299    , 0.8293    ,\n",
              "        0.83019999, 0.8295    , 0.8291    , 0.8293    , 0.8293    ,\n",
              "        0.8288    , 0.82980001, 0.8281    , 0.82900001, 0.8291    ,\n",
              "        0.8295    , 0.82970001, 0.8299    , 0.8297    , 0.8298    ,\n",
              "        0.82960001, 0.8295    , 0.83      , 0.82970001, 0.82960001,\n",
              "        0.8298    , 0.82970001, 0.82970001, 0.82970001, 0.8298    ,\n",
              "        0.82970001, 0.8295    , 0.8297    , 0.8297    , 0.8296    ,\n",
              "        0.8291    , 0.8295    , 0.8297    , 0.8298    , 0.83      ,\n",
              "        0.83      , 0.8293    , 0.82940001, 0.82950001, 0.8299    ]),\n",
              " array([0.55185659, 0.70890685, 0.77458787, 0.81354033, 0.84244958,\n",
              "        0.86751761, 0.88494318, 0.90298896, 0.91927417, 0.93577945,\n",
              "        0.94634283, 0.95710627, 0.96598912, 0.97347151, 0.9778129 ,\n",
              "        0.98147407, 0.98459507, 0.98789613, 0.98921655, 0.99175736,\n",
              "        0.99213748, 0.99361796, 0.99443822, 0.99551857, 0.99503841,\n",
              "        0.9959387 , 0.99615877, 0.99573864, 0.99663892, 0.99715909,\n",
              "        0.99723912, 0.9934379 , 0.99797935, 0.99943982, 0.99961988,\n",
              "        0.99985996, 0.99985996, 0.99991997, 1.        , 0.99995999,\n",
              "        0.99991997, 1.        , 0.99997999, 1.        , 1.        ,\n",
              "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "        0.99995999, 0.99983995, 0.99979994, 0.99987996, 0.99991997,\n",
              "        0.99991997, 0.99995999, 0.99989997, 0.99997999, 0.99991997,\n",
              "        1.        , 0.99989997, 0.99997999, 0.99997999, 0.99993998,\n",
              "        0.99995999, 0.99995999, 0.99997999, 0.99993998, 1.        ,\n",
              "        1.        , 0.99991997, 0.99997999, 0.99993998, 0.99997999,\n",
              "        0.99991997, 0.99993998, 0.99995999, 0.99995999, 0.99995999,\n",
              "        0.99995999, 0.99997999, 0.99997999, 0.99997999, 0.99985996,\n",
              "        0.99997999, 0.99995999, 0.99993998, 0.99991997, 0.99997999,\n",
              "        0.99995999, 0.99997999, 0.99995999, 0.99989997, 0.99995999,\n",
              "        0.99995999, 1.        , 0.99997999, 0.99997999, 0.99989997]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_0BYZzGnNbM",
        "colab_type": "code",
        "outputId": "589681c3-9beb-4ba5-8bff-ecf20ace125b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "!zip -r data_graph my_model_final.*"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: my_model_final.ckpt.data-00000-of-00001 (deflated 8%)\n",
            "  adding: my_model_final.ckpt.index (deflated 61%)\n",
            "  adding: my_model_final.ckpt.meta (deflated 91%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVUyqdH4nYxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('data_graph.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC_2T_1pny1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}